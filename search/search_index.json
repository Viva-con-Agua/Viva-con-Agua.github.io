{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Welcome to the documentation page for Viva con Agua IT projects. On this page you, can find information about the general structure and architecture of our applications and servers. This documentation is generally split into Tutorials to get you started. How-to-Guides to teach you a specific thing. References where you can lookup details about something you generally already know. Explanations for information about why we do something the way we do.","title":"Home"},{"location":"#home","text":"Welcome to the documentation page for Viva con Agua IT projects. On this page you, can find information about the general structure and architecture of our applications and servers. This documentation is generally split into Tutorials to get you started. How-to-Guides to teach you a specific thing. References where you can lookup details about something you generally already know. Explanations for information about why we do something the way we do.","title":"Home"},{"location":"explanations/","text":"Introducing Explanations This section of the documentation aim to explain to you not how but why something is the way it is. Missing There are not yet any explanations. Remove this note if you add an explanation.","title":"Introducing Explanations"},{"location":"explanations/#introducing-explanations","text":"This section of the documentation aim to explain to you not how but why something is the way it is. Missing There are not yet any explanations. Remove this note if you add an explanation.","title":"Introducing Explanations"},{"location":"explanations/kubernetes/","text":"The what and why of Kubernetes Missing This document has not yet been written.","title":"The what and why of Kubernetes"},{"location":"explanations/kubernetes/#the-what-and-why-of-kubernetes","text":"Missing This document has not yet been written.","title":"The what and why of Kubernetes"},{"location":"how-to-guides/","text":"Introducing How-To-Guides This section of the documentation aims to show you how a specific goal can be accomplished. You can see a list of guides on the left sidebar; pick one from there. What to read If you are wondering how to do a certain thing, the following guides might help you. They can also be reached via the navigation tree on the left. Writing Documentation teaches you about the system used to generate the documentation you are currently reading. The Deployment series describes how any application or part of an application can be brought from just source code to be deployed on our servers. The DonationForm teaches you about set up an donation-form.","title":"Introducing How-To-Guides"},{"location":"how-to-guides/#introducing-how-to-guides","text":"This section of the documentation aims to show you how a specific goal can be accomplished. You can see a list of guides on the left sidebar; pick one from there.","title":"Introducing How-To-Guides"},{"location":"how-to-guides/#what-to-read","text":"If you are wondering how to do a certain thing, the following guides might help you. They can also be reached via the navigation tree on the left. Writing Documentation teaches you about the system used to generate the documentation you are currently reading. The Deployment series describes how any application or part of an application can be brought from just source code to be deployed on our servers. The DonationForm teaches you about set up an donation-form.","title":"What to read"},{"location":"how-to-guides/writing_documentation/","text":"Writing Documentation The documentation you are currently viewing is based on MkDocs which is a simple markdown based documentation system. We use it because of its low maintenance burden, high popularity and ease of deployment via GitHub pages. If you are looking for upstream documentation, look here: MkDocs Writing your docs MkDocs general User Guide Material for MkDocs User Guide Material for MkDocs Reference Adding a new page Adding a new page is very easy since you only need to create a markdown file inside the docs folder (see project layout below) and then create a link to it for navigation (inside mkdocs.yml ). It generally does not matter where you place your file but we prefer to keep the files in the same structure as the navigation sections. Tutorials are used to teach a reader a new subject or lesson. It is generally written in easy language and allows the newcomer to get started. See here on how to write good tutorials. How-to guides serve to show how a specific goal can be accomplished like this page shows you how to create a new doc page. See here on how to write good guides. References are informative in nature and serve describe a specific solution in detail. Often, reference documentation can be found not here but as inline docstrings directly in software packages. See here on how to write good references. Explanations explain why something is done the way that it is done. See here on how to write good explanations. Generally refer to the Divio documentation system on how to write good documentation. Deployment The built documentation is automatically deployed on GitHub pages at viva-con-agua.github.io . There is nothing you need to do to re-deploy it except push your changes to GitHub. Project layout 1 2 3 4 5 6 mkdocs.yml # The MkDocs configuration file docs/ home/ # A section folder for the \"Home\" section index.md # The documentation homepage documentation_system.md # The page you are currently vewing ... # Other markdown pages, images and other files","title":"Writing Documentation"},{"location":"how-to-guides/writing_documentation/#writing-documentation","text":"The documentation you are currently viewing is based on MkDocs which is a simple markdown based documentation system. We use it because of its low maintenance burden, high popularity and ease of deployment via GitHub pages. If you are looking for upstream documentation, look here: MkDocs Writing your docs MkDocs general User Guide Material for MkDocs User Guide Material for MkDocs Reference","title":"Writing Documentation"},{"location":"how-to-guides/writing_documentation/#adding-a-new-page","text":"Adding a new page is very easy since you only need to create a markdown file inside the docs folder (see project layout below) and then create a link to it for navigation (inside mkdocs.yml ). It generally does not matter where you place your file but we prefer to keep the files in the same structure as the navigation sections. Tutorials are used to teach a reader a new subject or lesson. It is generally written in easy language and allows the newcomer to get started. See here on how to write good tutorials. How-to guides serve to show how a specific goal can be accomplished like this page shows you how to create a new doc page. See here on how to write good guides. References are informative in nature and serve describe a specific solution in detail. Often, reference documentation can be found not here but as inline docstrings directly in software packages. See here on how to write good references. Explanations explain why something is done the way that it is done. See here on how to write good explanations. Generally refer to the Divio documentation system on how to write good documentation.","title":"Adding a new page"},{"location":"how-to-guides/writing_documentation/#deployment","text":"The built documentation is automatically deployed on GitHub pages at viva-con-agua.github.io . There is nothing you need to do to re-deploy it except push your changes to GitHub.","title":"Deployment"},{"location":"how-to-guides/writing_documentation/#project-layout","text":"1 2 3 4 5 6 mkdocs.yml # The MkDocs configuration file docs/ home/ # A section folder for the \"Home\" section index.md # The documentation homepage documentation_system.md # The page you are currently vewing ... # Other markdown pages, images and other files","title":"Project layout"},{"location":"how-to-guides/deployment/1_containers/","text":"How to create Containers Containers are a way to package and distribute software as packages that run very independently of the host operating system. Containers do not care whether you run them on Windows, macOS, Debian (Linux), in the cloud or in any other way. This makes them very useful and easy to deploy. Goal At the end of this guide, you will have a container that serves a simple Vue application to your browser. Prerequisites This guide assumes that you have some Vue application in your current working directory. Simply running npm init vue@latest is fine and changing into the generated directory is fine though. 1) Create a Dockerfile As said above, containers are a way to package your application. Dockerfile s in essence are scripts that tell your computer how such a container is built from the surrounding source code. The syntax reference and list of commands can be found here . Simply add a file called Dockerfile to the root of your application (besides package.json ) and fill it with the following content: Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # create an intermediate container named 'build' based on nodejs FROM docker.io/node:18-alpine AS build # switch the working directory to /app/src WORKDIR /app/src # add all files of the current directory to /app/src/ ADD ./ /app/src/ # install all required dependencies from package-lock.json and abort if it is not up-to-date RUN npm clean-install # build static files into dist folder (this is standard vue behavior) RUN npm run build # create another container named 'http' based on a stable version of the nginx webserver FROM docker.io/nginx:mainline as http # add a config file for the nginx webserver ADD .docker/nginx.conf /etc/nginx/conf.d/default.conf # add built files from the intermediate 'build' container into /var/www/ so that they can be served by nginx COPY --from = build /app/src/dist/ /var/www/ # add some additional image metadata # this is not strictly required but is nice so that the image describes itself EXPOSE 80/tcp 2) Create an nginx configuration In the Dockerfile we referenced the .docker/nginx.conf file. This file does not yet exist so docker would complain about that when building the container. What this file does is tell nginx how to serve our application. It can include a number of directives 1 and you can find more information on it in its documentation here . Info We generally prefer to keep files that are not directly part of the application source code and which are only required when deploying via docker in the .docker folder. .docker/nginx.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 server { # respond to all requests on port 80 with this configuration listen 80 ; server_name default_server ; # configure /var/www/ as the directory from which to serve static files as responses root /var/www ; # fall back to /index.html when a url does not have a corresponding file on the server try_files $uri $uri/ /index.html ; location /assets/ { # allow caching for 7 days and tell caches that the resource is immutable add_header \"Cache-Control\" \"max-age=604800, immutable\" ; } } 3) Create a .dockerignore file Inside our Dockerfile we clean-install all dependencies and then build the application from scratch. However, before that we are adding all files of the current directory to the container ( ADD ./ /app/src/ ) including the existing node_modules and dist folders that a developer might have on their machine. To prevent this we could either use a more fine-grained ADD instruction to only add actually required files, but we can also restrict docker from ever seeing certain files. This can be done by using a .dockerignore file. Its syntax 2 is similar to gitignore , and it completely hides certain files from docker. More fine-grained ADD instruction The part of the Dockerfile containing ADD ./ /app/src/ could be replaced by the following. As you can see this is a lot more verbose and requires updating if more top-level files or folders are added. Dockerfile 1 2 3 ADD package.json package-lock.json vite.config.js index.html /app/src/ ADD src /app/src/src/ ADD public /app/src/public/ .dockerignore 1 2 node_modules dist 4) Build and run the containerized app Now that we have everything in place we want to build the container and test that everything works correctly. Building First we need to build the container. This can be done using the docker build 3 command. Build the container docker build -t example_app . Command part Purpose docker build Call docker in build mode to build a container. -t example_app Gives the built container the tag example_app . This can be almost anything you want and should probably be the name of your app. . Uses the current directory as build context (meaning references to files from Dockerfile are relative to this directory). Running After docker build completed successfully you can start the container using the tag that was chosen in the build command by calling docker run 3 . Run the container docker run -p 8000 :80 example_app Command part Purpose docker run Call docker in run mode to start a container -p 8000:80 Publishes the given container port so that requests to your host machines port 8000 are forwarded to the containers port 80 . example_app The tag of the container that we used in the docker build step. Accessing Once the container has started up you can access the app at localhost:8000 since port 8000 is forwarded to the container and we have a webserver running inside it. Stopping You can stop the running container by pressing CTRL+C in the terminal. List of nginx directives \u21a9 .dockerignore Syntax Reference \u21a9 Docker Command Reference \u21a9 \u21a9","title":"1. Containers"},{"location":"how-to-guides/deployment/1_containers/#how-to-create-containers","text":"Containers are a way to package and distribute software as packages that run very independently of the host operating system. Containers do not care whether you run them on Windows, macOS, Debian (Linux), in the cloud or in any other way. This makes them very useful and easy to deploy. Goal At the end of this guide, you will have a container that serves a simple Vue application to your browser. Prerequisites This guide assumes that you have some Vue application in your current working directory. Simply running npm init vue@latest is fine and changing into the generated directory is fine though.","title":"How to create Containers"},{"location":"how-to-guides/deployment/1_containers/#1-create-a-dockerfile","text":"As said above, containers are a way to package your application. Dockerfile s in essence are scripts that tell your computer how such a container is built from the surrounding source code. The syntax reference and list of commands can be found here . Simply add a file called Dockerfile to the root of your application (besides package.json ) and fill it with the following content: Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # create an intermediate container named 'build' based on nodejs FROM docker.io/node:18-alpine AS build # switch the working directory to /app/src WORKDIR /app/src # add all files of the current directory to /app/src/ ADD ./ /app/src/ # install all required dependencies from package-lock.json and abort if it is not up-to-date RUN npm clean-install # build static files into dist folder (this is standard vue behavior) RUN npm run build # create another container named 'http' based on a stable version of the nginx webserver FROM docker.io/nginx:mainline as http # add a config file for the nginx webserver ADD .docker/nginx.conf /etc/nginx/conf.d/default.conf # add built files from the intermediate 'build' container into /var/www/ so that they can be served by nginx COPY --from = build /app/src/dist/ /var/www/ # add some additional image metadata # this is not strictly required but is nice so that the image describes itself EXPOSE 80/tcp","title":"1) Create a Dockerfile"},{"location":"how-to-guides/deployment/1_containers/#2-create-an-nginx-configuration","text":"In the Dockerfile we referenced the .docker/nginx.conf file. This file does not yet exist so docker would complain about that when building the container. What this file does is tell nginx how to serve our application. It can include a number of directives 1 and you can find more information on it in its documentation here . Info We generally prefer to keep files that are not directly part of the application source code and which are only required when deploying via docker in the .docker folder. .docker/nginx.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 server { # respond to all requests on port 80 with this configuration listen 80 ; server_name default_server ; # configure /var/www/ as the directory from which to serve static files as responses root /var/www ; # fall back to /index.html when a url does not have a corresponding file on the server try_files $uri $uri/ /index.html ; location /assets/ { # allow caching for 7 days and tell caches that the resource is immutable add_header \"Cache-Control\" \"max-age=604800, immutable\" ; } }","title":"2) Create an nginx configuration"},{"location":"how-to-guides/deployment/1_containers/#3-create-a-dockerignore-file","text":"Inside our Dockerfile we clean-install all dependencies and then build the application from scratch. However, before that we are adding all files of the current directory to the container ( ADD ./ /app/src/ ) including the existing node_modules and dist folders that a developer might have on their machine. To prevent this we could either use a more fine-grained ADD instruction to only add actually required files, but we can also restrict docker from ever seeing certain files. This can be done by using a .dockerignore file. Its syntax 2 is similar to gitignore , and it completely hides certain files from docker. More fine-grained ADD instruction The part of the Dockerfile containing ADD ./ /app/src/ could be replaced by the following. As you can see this is a lot more verbose and requires updating if more top-level files or folders are added. Dockerfile 1 2 3 ADD package.json package-lock.json vite.config.js index.html /app/src/ ADD src /app/src/src/ ADD public /app/src/public/ .dockerignore 1 2 node_modules dist","title":"3) Create a .dockerignore file"},{"location":"how-to-guides/deployment/1_containers/#4-build-and-run-the-containerized-app","text":"Now that we have everything in place we want to build the container and test that everything works correctly.","title":"4) Build and run the containerized app"},{"location":"how-to-guides/deployment/1_containers/#building","text":"First we need to build the container. This can be done using the docker build 3 command. Build the container docker build -t example_app . Command part Purpose docker build Call docker in build mode to build a container. -t example_app Gives the built container the tag example_app . This can be almost anything you want and should probably be the name of your app. . Uses the current directory as build context (meaning references to files from Dockerfile are relative to this directory).","title":"Building"},{"location":"how-to-guides/deployment/1_containers/#running","text":"After docker build completed successfully you can start the container using the tag that was chosen in the build command by calling docker run 3 . Run the container docker run -p 8000 :80 example_app Command part Purpose docker run Call docker in run mode to start a container -p 8000:80 Publishes the given container port so that requests to your host machines port 8000 are forwarded to the containers port 80 . example_app The tag of the container that we used in the docker build step.","title":"Running"},{"location":"how-to-guides/deployment/1_containers/#accessing","text":"Once the container has started up you can access the app at localhost:8000 since port 8000 is forwarded to the container and we have a webserver running inside it.","title":"Accessing"},{"location":"how-to-guides/deployment/1_containers/#stopping","text":"You can stop the running container by pressing CTRL+C in the terminal. List of nginx directives \u21a9 .dockerignore Syntax Reference \u21a9 Docker Command Reference \u21a9 \u21a9","title":"Stopping"},{"location":"how-to-guides/deployment/2_kubernetes/","text":"How to create Kubernetes manifests In part 1 of this series we have created a container image that includes a Vue application. This part now describes how that container can be deployed in a Kubernetes environment. For a detailed description of what kubernetes does and why we use it see the relevant explanation here . Similar to how containers should not be fine-tuned to our infrastructure and could be reused by others, the manifests that are created in this guide are also intended to be general. Goal After following this guide, you will have a series of Kubernetes manifests that describe how the application is deployed. Prerequisites This guide assumes that you already have a working container image for your application. Additionally your app should only respond to HTTP traffic without requiring any backing services like databases or persistent storage. 1) Create a deployment First we create a deployment which manages the lifecycle of our container. To do that create the file .k8s/deployment.yml in your app folder. Reading the Kubernetes documentation about describing objects and about deployments as well as referencing the list of available deployment manifest fields will help you understand how the manifest file is structured and what the used fields do. Info We generally prefer to keep files that are not directly part of the application source code and which are only required as part of kubernetes manifests inside the .k8s folder. .k8s/deployment.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : apps/v1 kind : Deployment metadata : name : example-app labels : &labels app.kubernetes.io/name : example-app app.kubernetes.io/component : frontend spec : selector : matchLabels : *labels template : metadata : labels : *labels spec : containers : - name : example-app image : ghcr.io/viva-con-agua/example-app imagePullPolicy : Always ports : - name : http containerPort : 80 2) Create a service In addition to the deployment our app also needs a service to route traffic to the created containers. The relevant Kubernetes documentation is the one about services and the list of available service manifest fields . .k8s/service.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Service metadata : name : example-app labels : &labels app.kubernetes.io/name : example-app app.kubernetes.io/component : frontend spec : selector : *labels ports : - name : http port : 80 targetPort : http 3) Bundle manifests together At this point all strictly required manifests exist, but we still want to bundle them together so that they can be applied as one . To do that, we create a kustomization.yml file. It is the configuration file of kustomize , and you should see the kustomization reference about the used fields. kustomization.yml 1 2 3 4 5 6 7 apiVersion : kustomize.config.k8s.io/v1beta1 commonLabels : app.kubernetes.io/part-of : example-app resources : - .k8s/deployment.yml - .k8s/service.yml What this file achieves is the following: Bundle the manifests together Add the label defined via commonLabels to all resources as well as all relevant resource selectors","title":"2. Kubernetes"},{"location":"how-to-guides/deployment/2_kubernetes/#how-to-create-kubernetes-manifests","text":"In part 1 of this series we have created a container image that includes a Vue application. This part now describes how that container can be deployed in a Kubernetes environment. For a detailed description of what kubernetes does and why we use it see the relevant explanation here . Similar to how containers should not be fine-tuned to our infrastructure and could be reused by others, the manifests that are created in this guide are also intended to be general. Goal After following this guide, you will have a series of Kubernetes manifests that describe how the application is deployed. Prerequisites This guide assumes that you already have a working container image for your application. Additionally your app should only respond to HTTP traffic without requiring any backing services like databases or persistent storage.","title":"How to create Kubernetes manifests"},{"location":"how-to-guides/deployment/2_kubernetes/#1-create-a-deployment","text":"First we create a deployment which manages the lifecycle of our container. To do that create the file .k8s/deployment.yml in your app folder. Reading the Kubernetes documentation about describing objects and about deployments as well as referencing the list of available deployment manifest fields will help you understand how the manifest file is structured and what the used fields do. Info We generally prefer to keep files that are not directly part of the application source code and which are only required as part of kubernetes manifests inside the .k8s folder. .k8s/deployment.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : apps/v1 kind : Deployment metadata : name : example-app labels : &labels app.kubernetes.io/name : example-app app.kubernetes.io/component : frontend spec : selector : matchLabels : *labels template : metadata : labels : *labels spec : containers : - name : example-app image : ghcr.io/viva-con-agua/example-app imagePullPolicy : Always ports : - name : http containerPort : 80","title":"1) Create a deployment"},{"location":"how-to-guides/deployment/2_kubernetes/#2-create-a-service","text":"In addition to the deployment our app also needs a service to route traffic to the created containers. The relevant Kubernetes documentation is the one about services and the list of available service manifest fields . .k8s/service.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Service metadata : name : example-app labels : &labels app.kubernetes.io/name : example-app app.kubernetes.io/component : frontend spec : selector : *labels ports : - name : http port : 80 targetPort : http","title":"2) Create a service"},{"location":"how-to-guides/deployment/2_kubernetes/#3-bundle-manifests-together","text":"At this point all strictly required manifests exist, but we still want to bundle them together so that they can be applied as one . To do that, we create a kustomization.yml file. It is the configuration file of kustomize , and you should see the kustomization reference about the used fields. kustomization.yml 1 2 3 4 5 6 7 apiVersion : kustomize.config.k8s.io/v1beta1 commonLabels : app.kubernetes.io/part-of : example-app resources : - .k8s/deployment.yml - .k8s/service.yml What this file achieves is the following: Bundle the manifests together Add the label defined via commonLabels to all resources as well as all relevant resource selectors","title":"3) Bundle manifests together"},{"location":"how-to-guides/deployment/3_ci_cd/","text":"How to configure deployment CI/CD In parts 1 and 2 we have created a container and corresponding kubernetes manifests which describe the deployment of that container. This guide now focuses on how those manifests can be deployed and updated automatically. To do this we will use a mixture of GitHub Actions for Continuous Integration and ArgoCD for Continuous Deployment. We will also set up our three stages prod , stage and dev . Goal After following this guide, you will have automatically deployed an example application in three stages prod , stage and dev . Prerequisites This guide assumes that you already have a repository with a working Dockerfile as well as generic manifests that just need to be deployed. 1) Build Container automatically First we need to set up CI for our application repository so that its docker image is built automatically. To do this, create the file .github/workflows/container_image.yml and fill it with the following content. Info For more details about the workflow file see the Github Action Syntax Reference as well as our repositoriy of reusable workflows . .github/workflows/contianer_image.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 name : container_image on : push : branches : - main - stage - develop jobs : build-container : uses : Viva-con-Agua/workflows/.github/workflows/build_image.yml@main deploy : needs : [ build-container ] uses : Viva-con-Agua/workflows/.github/workflows/deploy.yml@main secrets : inherit with : image_name : ${{ needs.build-container.outputs.image_name }} new_digest : ${{ needs.build-container.outputs.image_digest }} What this workflow achieves is: run the workflow whenever a change is pushed to one of the branches main , stage or develop build a container image and automatically tag it based on the branch deploy the just-built image 2) Create -deploy repository Step 1) just deploys the image however what this actually means is that it updates the digest 1 which should be deployed in another repository. We call this other repository the -deploy repository. So, in order for GitHub Actions to update the digest the -deploy repository needs to actually exist. Important The -deploy repository should be named like your app repository in addition to a -deploy postfix. 2.1) Create dev stage folder Each -deploy repository is structured to contain one folder for each deployment stage. We start with the dev stage which means you simply need to create a folder called dev . Create dev folder 1 mkdir dev 2.2) Create additional manifests Remember that in part 2 we created kubernetes manifests that are very general and don't include any deployment details. This means that such deployment details need to be added now since the -deploy repositories different stage folders are exactly where such details are known. Ingress Namespace An Ingress definition is very important because it defines how our app is reachable via http from the outside network. A simple manifest that terminates all traffic to https://example-app.dev.vivaconagua.org (and http with automatic redirects) and then forwards it to the service called example-app can be seen below. Info Relevant Kubernetes documentation is about Ingresses , backing services and available ingress manifest fields . dev/ingress.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-app annotations : cert-manager.io/cluster-issuer : letsencrypt-prod spec : tls : - hosts : [ example-app.dev.vivaconagua.org ] secretName : example-app.dev.vivaconagua.org-tls rules : - host : example-app.dev.vivaconagua.org http : paths : - path : / pathType : Prefix backend : service : name : example-app port : name : http We also want to put all resources into their own namespace to keep things organized. To do so simply create the manifest file that can be seen below. Info See relevent Kubernetes namespace documentation and available namespace manifest fields . 1 2 3 4 apiVersion : v1 kind : Namespace metadata : name : example-app--dev 2.4) Write dev kustomization Now that a folder for the dev stage exists and contains all deployment relevant manifests we just need to bundle them together so that the whole stage can be deployed in unison. dev/kustomization.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : example-app--dev commonLabels : app.kubernetes.io/part-of : example-app app.kubernetes.io/instance : dev resources : - https://github.com/Viva-con-Agua/example-app.git?ref=develop - ingress.yml - namespace.yml images : [] 3) Register app with ArgoCD The app is now theoretically completely deployable, but it still needs to actually be deployed. This is done using ArgoCD 2 . All our applications are defined inside the repository github.com/Viva-con-Agua/argocd-apps . Add a manifest for your application definition like the following. This manifest will instruct ArgoCD to scan for folders inside the -deploy repository and creates an appropriate application definition for every folder it finds. Since our -deploy repositories contain one folder for each deployed stage, ArgoCD creates a separate application for each stage. Info The relevant documentation is from ArgoCD about Applications and ApplicationSets . vca-apps/example-app.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion : argoproj.io/v1alpha1 kind : ApplicationSet metadata : name : example-app spec : generators : - git : repoURL : https://github.com/Viva-con-Agua/example-app-deploy.git revision : HEAD directories : - path : \"*\" template : metadata : name : \"example-app--{{path}}\" spec : destination : name : in-cluster project : \"vca-{{path}}\" source : repoURL : https://github.com/Viva-con-Agua/example-app-deploy.git targetRevision : HEAD path : \"{{path}}\" syncPolicy : automated : prune : true selfHeal : true Additionally don't forget to also include this app definition in the repositories kustomization.yml kustomization.yml 1 2 3 4 5 6 7 8 9 10 apiVersion : kustomize.config.k8s.io/v1beta1 namespace : argocd resources : - basic-cluster/project.yml - basic-cluster/argocd-apps.yml - \u2026 - vca-apps/projects.yml - vca-apps/example-app.yml - \u2026 4) Monitor status on ArgoCD As said above, the actual deployment is now done by ArgoCD, but you can monitor it over at https://argocd.vivaconagua.org . Digests are a content-addressable identifier of an image. As long as the input used to generate the image is unchanged, the digest value is predictable. \u21a9 See our ArgoCD reference about how we have set it up and how it works. \u21a9","title":"3. CI/CD"},{"location":"how-to-guides/deployment/3_ci_cd/#how-to-configure-deployment-cicd","text":"In parts 1 and 2 we have created a container and corresponding kubernetes manifests which describe the deployment of that container. This guide now focuses on how those manifests can be deployed and updated automatically. To do this we will use a mixture of GitHub Actions for Continuous Integration and ArgoCD for Continuous Deployment. We will also set up our three stages prod , stage and dev . Goal After following this guide, you will have automatically deployed an example application in three stages prod , stage and dev . Prerequisites This guide assumes that you already have a repository with a working Dockerfile as well as generic manifests that just need to be deployed.","title":"How to configure deployment CI/CD"},{"location":"how-to-guides/deployment/3_ci_cd/#1-build-container-automatically","text":"First we need to set up CI for our application repository so that its docker image is built automatically. To do this, create the file .github/workflows/container_image.yml and fill it with the following content. Info For more details about the workflow file see the Github Action Syntax Reference as well as our repositoriy of reusable workflows . .github/workflows/contianer_image.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 name : container_image on : push : branches : - main - stage - develop jobs : build-container : uses : Viva-con-Agua/workflows/.github/workflows/build_image.yml@main deploy : needs : [ build-container ] uses : Viva-con-Agua/workflows/.github/workflows/deploy.yml@main secrets : inherit with : image_name : ${{ needs.build-container.outputs.image_name }} new_digest : ${{ needs.build-container.outputs.image_digest }} What this workflow achieves is: run the workflow whenever a change is pushed to one of the branches main , stage or develop build a container image and automatically tag it based on the branch deploy the just-built image","title":"1) Build Container automatically"},{"location":"how-to-guides/deployment/3_ci_cd/#2-create-deploy-repository","text":"Step 1) just deploys the image however what this actually means is that it updates the digest 1 which should be deployed in another repository. We call this other repository the -deploy repository. So, in order for GitHub Actions to update the digest the -deploy repository needs to actually exist. Important The -deploy repository should be named like your app repository in addition to a -deploy postfix.","title":"2) Create -deploy repository"},{"location":"how-to-guides/deployment/3_ci_cd/#21-create-dev-stage-folder","text":"Each -deploy repository is structured to contain one folder for each deployment stage. We start with the dev stage which means you simply need to create a folder called dev . Create dev folder 1 mkdir dev","title":"2.1) Create dev stage folder"},{"location":"how-to-guides/deployment/3_ci_cd/#22-create-additional-manifests","text":"Remember that in part 2 we created kubernetes manifests that are very general and don't include any deployment details. This means that such deployment details need to be added now since the -deploy repositories different stage folders are exactly where such details are known. Ingress Namespace An Ingress definition is very important because it defines how our app is reachable via http from the outside network. A simple manifest that terminates all traffic to https://example-app.dev.vivaconagua.org (and http with automatic redirects) and then forwards it to the service called example-app can be seen below. Info Relevant Kubernetes documentation is about Ingresses , backing services and available ingress manifest fields . dev/ingress.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-app annotations : cert-manager.io/cluster-issuer : letsencrypt-prod spec : tls : - hosts : [ example-app.dev.vivaconagua.org ] secretName : example-app.dev.vivaconagua.org-tls rules : - host : example-app.dev.vivaconagua.org http : paths : - path : / pathType : Prefix backend : service : name : example-app port : name : http We also want to put all resources into their own namespace to keep things organized. To do so simply create the manifest file that can be seen below. Info See relevent Kubernetes namespace documentation and available namespace manifest fields . 1 2 3 4 apiVersion : v1 kind : Namespace metadata : name : example-app--dev","title":"2.2) Create additional manifests"},{"location":"how-to-guides/deployment/3_ci_cd/#24-write-dev-kustomization","text":"Now that a folder for the dev stage exists and contains all deployment relevant manifests we just need to bundle them together so that the whole stage can be deployed in unison. dev/kustomization.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : example-app--dev commonLabels : app.kubernetes.io/part-of : example-app app.kubernetes.io/instance : dev resources : - https://github.com/Viva-con-Agua/example-app.git?ref=develop - ingress.yml - namespace.yml images : []","title":"2.4) Write dev kustomization"},{"location":"how-to-guides/deployment/3_ci_cd/#3-register-app-with-argocd","text":"The app is now theoretically completely deployable, but it still needs to actually be deployed. This is done using ArgoCD 2 . All our applications are defined inside the repository github.com/Viva-con-Agua/argocd-apps . Add a manifest for your application definition like the following. This manifest will instruct ArgoCD to scan for folders inside the -deploy repository and creates an appropriate application definition for every folder it finds. Since our -deploy repositories contain one folder for each deployed stage, ArgoCD creates a separate application for each stage. Info The relevant documentation is from ArgoCD about Applications and ApplicationSets . vca-apps/example-app.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion : argoproj.io/v1alpha1 kind : ApplicationSet metadata : name : example-app spec : generators : - git : repoURL : https://github.com/Viva-con-Agua/example-app-deploy.git revision : HEAD directories : - path : \"*\" template : metadata : name : \"example-app--{{path}}\" spec : destination : name : in-cluster project : \"vca-{{path}}\" source : repoURL : https://github.com/Viva-con-Agua/example-app-deploy.git targetRevision : HEAD path : \"{{path}}\" syncPolicy : automated : prune : true selfHeal : true Additionally don't forget to also include this app definition in the repositories kustomization.yml kustomization.yml 1 2 3 4 5 6 7 8 9 10 apiVersion : kustomize.config.k8s.io/v1beta1 namespace : argocd resources : - basic-cluster/project.yml - basic-cluster/argocd-apps.yml - \u2026 - vca-apps/projects.yml - vca-apps/example-app.yml - \u2026","title":"3) Register app with ArgoCD"},{"location":"how-to-guides/deployment/3_ci_cd/#4-monitor-status-on-argocd","text":"As said above, the actual deployment is now done by ArgoCD, but you can monitor it over at https://argocd.vivaconagua.org . Digests are a content-addressable identifier of an image. As long as the input used to generate the image is unchanged, the digest value is predictable. \u21a9 See our ArgoCD reference about how we have set it up and how it works. \u21a9","title":"4) Monitor status on ArgoCD"},{"location":"how-to-guides/deployment/4_health_readiness_checks/","text":"How to auto-restart failed apps Goal This shows how probes can be added to Kubernetes deployments so that the underlying application is automatically restarted or excluded from receiving requests. Prerequisites This guide assumes that you already have an application and a deployment manifest for it. Info Also see the upstream Kubernetes documentation about probes Generally speaking Kubernetes supports three kinds different kinds of probes. All of these are run by Kubernetes at regular (configurable) intervals to check the health and readiness of a container. Restart unhealthy apps The first probe we'll consider is called a liveness probe . Its job is to probe the container to check if it is still alive and restarts it when it is not. Adding a liveness probe is simple enough. In the following example an HTTP probe is added to check the /healthz route. Any status code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : example-liveness-http spec : containers : - name : liveness image : k8s.gcr.io/liveness args : - /server livenessProbe : httpGet : path : /healthz port : 8080 What makes a liveness check Liveness checks should only check the bare minimum of a running container. They should never include checks of required dependent services. They should also be configured with more generous failure thresholds than readiness probes. Starve unready apps Another useful probe is a readiness probe . Similar to liveness probes, Kubernetes regularly runs it to check whether the container is ready to accept traffic. If not, the container is removed from any upstream services and no new requests are delivered to it. In the following example an HTTP probe is configure to check the /readiness route. Any status code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : example-liveness-http spec : containers : - name : liveness image : k8s.gcr.io/liveness args : - /server readinessProbe : httpGet : path : /healthz port : 8080 What makes a readiness check Readiness checks should check whether the app could serve a request right now. It should however still not check shared dependencies 1 2 . Instead it should check internal state (e.g. request queue is not too long) and it may check availability of exclusive dependencies. Kubernetes Liveness and Readiness Probes: How to Avoid Shooting Yourself in the Foot \u21a9 Liveness Probes are Dangerous \u21a9","title":"4. Auto-Restart failed apps"},{"location":"how-to-guides/deployment/4_health_readiness_checks/#how-to-auto-restart-failed-apps","text":"Goal This shows how probes can be added to Kubernetes deployments so that the underlying application is automatically restarted or excluded from receiving requests. Prerequisites This guide assumes that you already have an application and a deployment manifest for it. Info Also see the upstream Kubernetes documentation about probes Generally speaking Kubernetes supports three kinds different kinds of probes. All of these are run by Kubernetes at regular (configurable) intervals to check the health and readiness of a container.","title":"How to auto-restart failed apps"},{"location":"how-to-guides/deployment/4_health_readiness_checks/#restart-unhealthy-apps","text":"The first probe we'll consider is called a liveness probe . Its job is to probe the container to check if it is still alive and restarts it when it is not. Adding a liveness probe is simple enough. In the following example an HTTP probe is added to check the /healthz route. Any status code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : example-liveness-http spec : containers : - name : liveness image : k8s.gcr.io/liveness args : - /server livenessProbe : httpGet : path : /healthz port : 8080 What makes a liveness check Liveness checks should only check the bare minimum of a running container. They should never include checks of required dependent services. They should also be configured with more generous failure thresholds than readiness probes.","title":"Restart unhealthy apps"},{"location":"how-to-guides/deployment/4_health_readiness_checks/#starve-unready-apps","text":"Another useful probe is a readiness probe . Similar to liveness probes, Kubernetes regularly runs it to check whether the container is ready to accept traffic. If not, the container is removed from any upstream services and no new requests are delivered to it. In the following example an HTTP probe is configure to check the /readiness route. Any status code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : example-liveness-http spec : containers : - name : liveness image : k8s.gcr.io/liveness args : - /server readinessProbe : httpGet : path : /healthz port : 8080 What makes a readiness check Readiness checks should check whether the app could serve a request right now. It should however still not check shared dependencies 1 2 . Instead it should check internal state (e.g. request queue is not too long) and it may check availability of exclusive dependencies. Kubernetes Liveness and Readiness Probes: How to Avoid Shooting Yourself in the Foot \u21a9 Liveness Probes are Dangerous \u21a9","title":"Starve unready apps"},{"location":"how-to-guides/deployment/5_dynamic_configuration/","text":"How to inject dynamic configuration Parts 1) through 3) of this guide focused on the bare minimum of deploying a static application on our infrastructure. Real word applications are not always serving static files though and might need some configuration that is not baked into the application container but only provided at runtime. This guide walks through how we do that. Goal Provide a environment variable to the containerized application via Kubernetes Prerequisites You should already have a basic deployment set up. This means that you should have: An application repository with CI/CD set up A -deploy repository 1)","title":"How to inject dynamic configuration"},{"location":"how-to-guides/deployment/5_dynamic_configuration/#how-to-inject-dynamic-configuration","text":"Parts 1) through 3) of this guide focused on the bare minimum of deploying a static application on our infrastructure. Real word applications are not always serving static files though and might need some configuration that is not baked into the application container but only provided at runtime. This guide walks through how we do that. Goal Provide a environment variable to the containerized application via Kubernetes Prerequisites You should already have a basic deployment set up. This means that you should have: An application repository with CI/CD set up A -deploy repository","title":"How to inject dynamic configuration"},{"location":"how-to-guides/deployment/5_dynamic_configuration/#1","text":"","title":"1)"},{"location":"how-to-guides/donation-form/1_create/","text":"How to create an donation form In order to be able to manage Viva con Agua's online donations, we maintain our own donation form. Each form gets its own ID and so it can be assigned to an campaign. This donation_form_id also determines other properties. For example the slider element or the default amount of an donation. A donation_form_id also refers to a specific company. This allows us to store the authentication information of the payment service providers in our backend. Goal At the end of this guide, you will have a donation form element created via factory service. Prerequisites This guide assumes that you have deployed the factory service or access to the live factory . For live deployment you need also access to the CRM . 1) Create an CRM Campaign TODO 2) Create an donation-form via Factory Login into Factory Go to Formular section. Open Formular Anlegen 3) Setting Param The setting param is used to define other \"donation modes\". key description nwt used for NWT registration payments v2 new layout (15.09.22) Example: 1 http://donation-form.vivaconagua.org/#/?donation_form_id=<id>&setting=v2","title":"1. Create"},{"location":"how-to-guides/donation-form/1_create/#how-to-create-an-donation-form","text":"In order to be able to manage Viva con Agua's online donations, we maintain our own donation form. Each form gets its own ID and so it can be assigned to an campaign. This donation_form_id also determines other properties. For example the slider element or the default amount of an donation. A donation_form_id also refers to a specific company. This allows us to store the authentication information of the payment service providers in our backend. Goal At the end of this guide, you will have a donation form element created via factory service. Prerequisites This guide assumes that you have deployed the factory service or access to the live factory . For live deployment you need also access to the CRM .","title":"How to create an donation form"},{"location":"how-to-guides/donation-form/1_create/#1-create-an-crm-campaign","text":"TODO","title":"1) Create an CRM Campaign"},{"location":"how-to-guides/donation-form/1_create/#2-create-an-donation-form-via-factory","text":"Login into Factory Go to Formular section. Open Formular Anlegen","title":"2) Create an donation-form via Factory"},{"location":"how-to-guides/donation-form/1_create/#3-setting-param","text":"The setting param is used to define other \"donation modes\". key description nwt used for NWT registration payments v2 new layout (15.09.22) Example: 1 http://donation-form.vivaconagua.org/#/?donation_form_id=<id>&setting=v2","title":"3) Setting Param"},{"location":"how-to-guides/donation-form/2_embedding/","text":"How to embedding an donation form into an website. In the most cases the donation-form will be embedded as an iframe. Goal At the end of this guide, you will have a donation-form running on a website. Prerequisites This guide assumes that you have basic html and js skills. Get an donation_form_id by create an donation_form. 1) Create onmessage For handling javascript events, we define at first an window.onmessage block: 1 2 3 4 5 6 7 8 9 10 11 <script type=\"text/javascript\"> window.onmessage = function(e) { if (e.data == 'navigate') { var top = document.getElementById('donation-form').offsetTop; window.scrollTo(0, top); } if (e.data.event == 'gtm-trigger') { dataLayer?.push(e.data.data) } }; </script> The navigate event is used for scroll to the top of the iframe. The gtm-trigger is used for handling GoogleTagManager events. 2) Resize Iframe To ensure that the iframe always fits correctly into the page, we need second script block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <script type=\"text/javascript\"> //stores the iframe elemement. var iframeElement = null //used after the iframe is loaded. const messageThatIframeIsLoaded = () => { //assign the donation form element to iframeElement iframeElement = document.getElementById('donation-form') if (iframeElement) { //trigger the message event in the iframe. iframeElement.contentWindow?.postMessage({ type: 'iframe-loaded' }, '*') } } //used for handle iframe resize. const handleIframeMessage = (event) => { if (event.data.type === 'iframe-height') { console.log('iframe event', event.origin, event.data.data) //resize iframeElement iframeElement.style.height = event.data.data.height +\"px\" } } //add event listener for handling the function call via message event. window.addEventListener('message', handleIframeMessage, false) </script> The iframeElement variable is for reference the iframe element. At load the messageThatIframeIsLoaded function will assign the element to this variable and post an iframe-loaded message to the iframe. This will trigger an iframe-height event that contains the current size of the iframe element. For handling this kind of events, we define the handleIframeMessage function. In case of an iframe-height event, the function will set the height of the iframe to the value of the event data. Finally an event listener for message is defined for handling the iframe-height event. 3) Iframe Finally, the iframe. The donation_form_id must be adjusted so that the correct campagne can be assigned. The messageThatIframeIsLoaded function is defined in the onload interface of the iframe, so the iframe is set directly to the correct size during initial loading. 1 <iframe id=\"donation-form\" onload=\"messageThatIframeIsLoaded()\" src=\"https://donation-form.vivaconagua.org/#/?donation_form_id=<donation_form_id>\" style=\"width: 99%; border: none; height: 1765px;\"></iframe>","title":"2. Embedding"},{"location":"how-to-guides/donation-form/2_embedding/#how-to-embedding-an-donation-form-into-an-website","text":"In the most cases the donation-form will be embedded as an iframe. Goal At the end of this guide, you will have a donation-form running on a website. Prerequisites This guide assumes that you have basic html and js skills. Get an donation_form_id by create an donation_form.","title":"How to embedding an donation form into an website."},{"location":"how-to-guides/donation-form/2_embedding/#1-create-onmessage","text":"For handling javascript events, we define at first an window.onmessage block: 1 2 3 4 5 6 7 8 9 10 11 <script type=\"text/javascript\"> window.onmessage = function(e) { if (e.data == 'navigate') { var top = document.getElementById('donation-form').offsetTop; window.scrollTo(0, top); } if (e.data.event == 'gtm-trigger') { dataLayer?.push(e.data.data) } }; </script> The navigate event is used for scroll to the top of the iframe. The gtm-trigger is used for handling GoogleTagManager events.","title":"1) Create onmessage"},{"location":"how-to-guides/donation-form/2_embedding/#2-resize-iframe","text":"To ensure that the iframe always fits correctly into the page, we need second script block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <script type=\"text/javascript\"> //stores the iframe elemement. var iframeElement = null //used after the iframe is loaded. const messageThatIframeIsLoaded = () => { //assign the donation form element to iframeElement iframeElement = document.getElementById('donation-form') if (iframeElement) { //trigger the message event in the iframe. iframeElement.contentWindow?.postMessage({ type: 'iframe-loaded' }, '*') } } //used for handle iframe resize. const handleIframeMessage = (event) => { if (event.data.type === 'iframe-height') { console.log('iframe event', event.origin, event.data.data) //resize iframeElement iframeElement.style.height = event.data.data.height +\"px\" } } //add event listener for handling the function call via message event. window.addEventListener('message', handleIframeMessage, false) </script> The iframeElement variable is for reference the iframe element. At load the messageThatIframeIsLoaded function will assign the element to this variable and post an iframe-loaded message to the iframe. This will trigger an iframe-height event that contains the current size of the iframe element. For handling this kind of events, we define the handleIframeMessage function. In case of an iframe-height event, the function will set the height of the iframe to the value of the event data. Finally an event listener for message is defined for handling the iframe-height event.","title":"2) Resize Iframe"},{"location":"how-to-guides/donation-form/2_embedding/#3-iframe","text":"Finally, the iframe. The donation_form_id must be adjusted so that the correct campagne can be assigned. The messageThatIframeIsLoaded function is defined in the onload interface of the iframe, so the iframe is set directly to the correct size during initial loading. 1 <iframe id=\"donation-form\" onload=\"messageThatIframeIsLoaded()\" src=\"https://donation-form.vivaconagua.org/#/?donation_form_id=<donation_form_id>\" style=\"width: 99%; border: none; height: 1765px;\"></iframe>","title":"3) Iframe"},{"location":"references/","text":"Introducing References This section of the documentation serves as lookup material and reference material. It is purely informative and assumes you already roughly know the subject as. Note Most of our reference material for code is located not here but as inline docstrings in the code itself.","title":"Introducing References"},{"location":"references/#introducing-references","text":"This section of the documentation serves as lookup material and reference material. It is purely informative and assumes you already roughly know the subject as. Note Most of our reference material for code is located not here but as inline docstrings in the code itself.","title":"Introducing References"},{"location":"references/app_metrics/","text":"Application Metrics Collection Our observability stack based on prometheus is configured to automatically discover and scrape application metrics from apps deployed on kubernetes. Viewing Metrics Just visit https://observability.vivaconagua.org/, and you will see our Grafana dashboard. There you can view one of the preexisting dashboards as well as explore all collected metrics. Exposing Metrics To expose metrics, read the official prometheus documentation about instrumenting an app . Finally, in order for metrics to be collected, the kubernetes manifests of that app need to be annotated correctly. You can annotate pods as well as services: If you annotate pods inside a deployment template, all pods of that deployment will get scraped. This might be useful for metrics that show e.g. how much memory an app process uses. If you annotate services, kubernetes will route the metrics collection request of prometheus to one random backing pod. This type of collection might be useful to collect aggregated application metrics e.g. number of registered users. Kubernetes Annotations Annotation Default Description prometheus.io/scrape false Must be set to true to enable scraping at all prometheus.io/metric_path /metrics HTTP path under which metrics are served prometheus.io/port the first port defined on the pod or service Which port prometheus scrapes for metrics prometheus.io/scheme http HTTP scheme which prometheus uses to scrape for metrics","title":"App Metrics"},{"location":"references/app_metrics/#application-metrics-collection","text":"Our observability stack based on prometheus is configured to automatically discover and scrape application metrics from apps deployed on kubernetes.","title":"Application Metrics Collection"},{"location":"references/app_metrics/#viewing-metrics","text":"Just visit https://observability.vivaconagua.org/, and you will see our Grafana dashboard. There you can view one of the preexisting dashboards as well as explore all collected metrics.","title":"Viewing Metrics"},{"location":"references/app_metrics/#exposing-metrics","text":"To expose metrics, read the official prometheus documentation about instrumenting an app . Finally, in order for metrics to be collected, the kubernetes manifests of that app need to be annotated correctly. You can annotate pods as well as services: If you annotate pods inside a deployment template, all pods of that deployment will get scraped. This might be useful for metrics that show e.g. how much memory an app process uses. If you annotate services, kubernetes will route the metrics collection request of prometheus to one random backing pod. This type of collection might be useful to collect aggregated application metrics e.g. number of registered users. Kubernetes Annotations Annotation Default Description prometheus.io/scrape false Must be set to true to enable scraping at all prometheus.io/metric_path /metrics HTTP path under which metrics are served prometheus.io/port the first port defined on the pod or service Which port prometheus scrapes for metrics prometheus.io/scheme http HTTP scheme which prometheus uses to scrape for metrics","title":"Exposing Metrics"},{"location":"references/nats/","text":"NATS We have deployed NATS on our infrastructure as a message broker to communicate between our different services. Its deployment configuration is located in the nats-deploy repository on GitHub and details about the deployment are described there. Connecting to NATS NATS is exposed to the rest of the cluster via a service object simply called nats inside its namespace. This means that connections are possible via the hostname associated to that service, e.g. nats.nats-dev.svc.cluster.local . NATS Hostname nats.nats-dev.svc.cluster.local but dev can also be stage or prod to use that NATS instance instead. Interactive Session from inside the Cluster kubectl supports running one-off pods in the cluster. We use this capability to run a nats-box pod and use the installed tools there. 1 kubectl run -it --rm nats-box --image = natsio/nats-box You can now verify the server connection by running: 1 nats -s nats.nats-dev.svc.cluster.local server check Port-Forwarding to local computer kubectl supports port-forwarding arbitrary services to your local computer. The following command port-forwards the port named nats-client on the service named nats inside the namespace nats-dev . 1 kubectl -n nats-dev port-forward svc/nats nats-client You can now verify the server connection by running: 1 nats -s localhost server check","title":"NATS"},{"location":"references/nats/#nats","text":"We have deployed NATS on our infrastructure as a message broker to communicate between our different services. Its deployment configuration is located in the nats-deploy repository on GitHub and details about the deployment are described there.","title":"NATS"},{"location":"references/nats/#connecting-to-nats","text":"NATS is exposed to the rest of the cluster via a service object simply called nats inside its namespace. This means that connections are possible via the hostname associated to that service, e.g. nats.nats-dev.svc.cluster.local . NATS Hostname nats.nats-dev.svc.cluster.local but dev can also be stage or prod to use that NATS instance instead.","title":"Connecting to NATS"},{"location":"references/nats/#interactive-session-from-inside-the-cluster","text":"kubectl supports running one-off pods in the cluster. We use this capability to run a nats-box pod and use the installed tools there. 1 kubectl run -it --rm nats-box --image = natsio/nats-box You can now verify the server connection by running: 1 nats -s nats.nats-dev.svc.cluster.local server check","title":"Interactive Session from inside the Cluster"},{"location":"references/nats/#port-forwarding-to-local-computer","text":"kubectl supports port-forwarding arbitrary services to your local computer. The following command port-forwards the port named nats-client on the service named nats inside the namespace nats-dev . 1 kubectl -n nats-dev port-forward svc/nats nats-client You can now verify the server connection by running: 1 nats -s localhost server check","title":"Port-Forwarding to local computer"},{"location":"references/secret_management/","text":"Secret Management Because we implement a GitOps style deployment and secrets should not be stored in plaintext in a git repository we require a form of secret management. To solve this, we use the pass password manager to store and manage all our secrets (not just passwords) encrypted in a private repository 1 . Kustomize Integration We have decided to retrieve secrets from pass during the Kubernetes manifest generation. Since our Kubernetes are generated with kustomize this means that kustomize needs to extract secrets during that step. To accomplish this we use kustomize-pass to transform references to secrets into their actual secret content. Example The following reference to a secret would be automatically looked up during the manifest generation and transformed into the kubernetes secret shown afterwards (assuming the referenced secret actually contains the example data): 1 2 3 4 5 6 7 8 9 10 11 # reference to secret apiVersion : ftsell.de/v1beta1 kind : PassSecret metadata : name : example-secret annotations : config.kubernetes.io/function : | exec: path: kustomize-pass data : example-key : example-pass-name 1 2 3 4 5 6 7 # generated kubernetes secret apiVersion : v1 kind : Secret metadata : name : example-secret data : example-key : foobar123 ArgoCD Integration ArgoCD performs our final deployment 2 . Naturally it needs to be able to resolve the references to pass secrets. To do this, a few patches to the default ArgoCD deployment have been applied: The deployed container image has been changed to include the kustomize-pass binary A secret gpg key for ArgoCD has been added to our pass password store and been provided to the running ArgoCD . The key has the id BC5FD89696FE550BE835D67478BFB0094BE70CBB and is stored in pass itself under services/basic-cluster/argocd/gpg.key . With these patches, ArgoCD can call kustomize normally which in turn is able to resolve the secrets as described above. https://github.com/Viva-con-Agua/pass/ \u21a9 See Continuous Delivery documentation \u21a9","title":"Secret Management"},{"location":"references/secret_management/#secret-management","text":"Because we implement a GitOps style deployment and secrets should not be stored in plaintext in a git repository we require a form of secret management. To solve this, we use the pass password manager to store and manage all our secrets (not just passwords) encrypted in a private repository 1 .","title":"Secret Management"},{"location":"references/secret_management/#kustomize-integration","text":"We have decided to retrieve secrets from pass during the Kubernetes manifest generation. Since our Kubernetes are generated with kustomize this means that kustomize needs to extract secrets during that step. To accomplish this we use kustomize-pass to transform references to secrets into their actual secret content. Example The following reference to a secret would be automatically looked up during the manifest generation and transformed into the kubernetes secret shown afterwards (assuming the referenced secret actually contains the example data): 1 2 3 4 5 6 7 8 9 10 11 # reference to secret apiVersion : ftsell.de/v1beta1 kind : PassSecret metadata : name : example-secret annotations : config.kubernetes.io/function : | exec: path: kustomize-pass data : example-key : example-pass-name 1 2 3 4 5 6 7 # generated kubernetes secret apiVersion : v1 kind : Secret metadata : name : example-secret data : example-key : foobar123","title":"Kustomize Integration"},{"location":"references/secret_management/#argocd-integration","text":"ArgoCD performs our final deployment 2 . Naturally it needs to be able to resolve the references to pass secrets. To do this, a few patches to the default ArgoCD deployment have been applied: The deployed container image has been changed to include the kustomize-pass binary A secret gpg key for ArgoCD has been added to our pass password store and been provided to the running ArgoCD . The key has the id BC5FD89696FE550BE835D67478BFB0094BE70CBB and is stored in pass itself under services/basic-cluster/argocd/gpg.key . With these patches, ArgoCD can call kustomize normally which in turn is able to resolve the secrets as described above. https://github.com/Viva-con-Agua/pass/ \u21a9 See Continuous Delivery documentation \u21a9","title":"ArgoCD Integration"},{"location":"references/software_architecture/","text":"Software Architecture Missing There is nothing here yet, this document only exists so that it can be linked to.","title":"Software Architecture"},{"location":"references/software_architecture/#software-architecture","text":"Missing There is nothing here yet, this document only exists so that it can be linked to.","title":"Software Architecture"},{"location":"references/devops/","text":"DevOps Overview Wikipedia on DevOps DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality. From this definition, three key principles are often derived: Shared Ownership, Workflow Automation and Rapid Feedback . In this document, I will explain how VcA implements these principles on an abstract level. For more specific details see the linked articles in the following sections or explore the sidebar. Shared Ownership One of the key principles of DevOps is that there exists a shared ownership of the problem domain between development and operations department. At VcA we currently only have one IT team so this part is naturally implemented\u2026 Generally we use specific services to solve small parts of our problems i.e. we use microservices. For more details visit the Software Architecture article. Workflow Automation Our workflows are currently entirely non-automated, but we are working on it. In preparation for proper CI/CD processes we are however already doing the following: All new services are built and deployed using containers . While deployment is not yet done automatically, with containers we can easily build and deploy a service manually. Rapid Feedback Our feedback cycles are generally not too long since we test our services ourselves and generally have good connections to other departments that give us feedback. This however in no way counts as rapid feedback and we currently do not have an established testing / linting / \u2026 methodology.","title":"DevOps Overview"},{"location":"references/devops/#devops-overview","text":"Wikipedia on DevOps DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality. From this definition, three key principles are often derived: Shared Ownership, Workflow Automation and Rapid Feedback . In this document, I will explain how VcA implements these principles on an abstract level. For more specific details see the linked articles in the following sections or explore the sidebar.","title":"DevOps Overview"},{"location":"references/devops/#shared-ownership","text":"One of the key principles of DevOps is that there exists a shared ownership of the problem domain between development and operations department. At VcA we currently only have one IT team so this part is naturally implemented\u2026 Generally we use specific services to solve small parts of our problems i.e. we use microservices. For more details visit the Software Architecture article.","title":"Shared Ownership"},{"location":"references/devops/#workflow-automation","text":"Our workflows are currently entirely non-automated, but we are working on it. In preparation for proper CI/CD processes we are however already doing the following: All new services are built and deployed using containers . While deployment is not yet done automatically, with containers we can easily build and deploy a service manually.","title":"Workflow Automation"},{"location":"references/devops/#rapid-feedback","text":"Our feedback cycles are generally not too long since we test our services ourselves and generally have good connections to other departments that give us feedback. This however in no way counts as rapid feedback and we currently do not have an established testing / linting / \u2026 methodology.","title":"Rapid Feedback"},{"location":"references/devops/cd/","text":"Continuous Delivery Using continuous delivery, we aim to deploy our software as soon as it is tested and verified in an automated process. What is GitOps? | GitLab There are four key components to a GitOps workflow, a Git repository, a continuous delivery (CD) pipeline, an application deployment tool, and a monitoring system. The Git repository is the source of truth for the application configuration and code. The CD pipeline is responsible for building, testing, and deploying the application. The deployment tool is used to manage the application resources in the target environment. The monitoring system tracks the application performance and provides feedback to the development team. In this document, we focus on points 1. and 3., the git repository and deployment tool . For us, the latter is ArgoCD (access here ). How ArgoCD works During any deployment process, a few steps are performed: A user changes something in a deployment repository that ArgoCD is configured to monitor. The repository provider (GitHub but others are possible too) sends a webhook to ArgoCD to notify it that something changed in this repository. The content of the event is mostly ignored by ArgoCD except that it triggers a refresh. ArgoCD performs a refresh of the repositories content. Note This step still works if no webhook is configured because ArgoCD regularly performs this refresh anyways. Nonetheless, an organization level webhook is configured for all our GitHub repositories. This refresh includes building the manifests contained in that repository which also includes fetching secrets from pass 1 . Based on the completely built manifests ArgoCD synchronizes the Kubernetes cluster to be as the manifests describe. One the application is deployed or updated, the user can access and use it. See the Secret Management documentation for details on how ArgoCD fetches secrets from pass. \u21a9","title":"Continuous Delivery"},{"location":"references/devops/cd/#continuous-delivery","text":"Using continuous delivery, we aim to deploy our software as soon as it is tested and verified in an automated process. What is GitOps? | GitLab There are four key components to a GitOps workflow, a Git repository, a continuous delivery (CD) pipeline, an application deployment tool, and a monitoring system. The Git repository is the source of truth for the application configuration and code. The CD pipeline is responsible for building, testing, and deploying the application. The deployment tool is used to manage the application resources in the target environment. The monitoring system tracks the application performance and provides feedback to the development team. In this document, we focus on points 1. and 3., the git repository and deployment tool . For us, the latter is ArgoCD (access here ).","title":"Continuous Delivery"},{"location":"references/devops/cd/#how-argocd-works","text":"During any deployment process, a few steps are performed: A user changes something in a deployment repository that ArgoCD is configured to monitor. The repository provider (GitHub but others are possible too) sends a webhook to ArgoCD to notify it that something changed in this repository. The content of the event is mostly ignored by ArgoCD except that it triggers a refresh. ArgoCD performs a refresh of the repositories content. Note This step still works if no webhook is configured because ArgoCD regularly performs this refresh anyways. Nonetheless, an organization level webhook is configured for all our GitHub repositories. This refresh includes building the manifests contained in that repository which also includes fetching secrets from pass 1 . Based on the completely built manifests ArgoCD synchronizes the Kubernetes cluster to be as the manifests describe. One the application is deployed or updated, the user can access and use it. See the Secret Management documentation for details on how ArgoCD fetches secrets from pass. \u21a9","title":"How ArgoCD works"},{"location":"references/devops/domain_deploy/","text":"Domain Deploy git Setup Get repo 1 git clone https://github.com/Viva-con-Agua/domain-deploy.git Create folder named subdomains and change to the repo root directory. 1 2 mkdir subdomains cd domain-deploy Create a new .env file and edit: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cp example.env .env subdomains=/path/to/subdomain cert=/path/to/cert ``` ## Setup Network ``` please network create please network info please network delete ``` ## Setup Domain ./please setup 1 ## Usage Usage: up # setup nginx rm # remove nginx logs # show nginx logs restart # restart nginx add # add subdomain link # connect subdomain with service edit # edit subdomain help # print help ``` Add subdomain Add subdomain to the services.ini file. ./please add test example.com Link subdomain Link service ./please link test Output: subdomain test.example.com is successfully link to docker with IP: 172.4.0.4 Use the IP as domain_ip in your service.","title":"Domain Deploy"},{"location":"references/devops/domain_deploy/#domain-deploy","text":"git","title":"Domain Deploy"},{"location":"references/devops/domain_deploy/#setup","text":"Get repo 1 git clone https://github.com/Viva-con-Agua/domain-deploy.git Create folder named subdomains and change to the repo root directory. 1 2 mkdir subdomains cd domain-deploy Create a new .env file and edit: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cp example.env .env subdomains=/path/to/subdomain cert=/path/to/cert ``` ## Setup Network ``` please network create please network info please network delete ``` ## Setup Domain ./please setup 1 ## Usage Usage: up # setup nginx rm # remove nginx logs # show nginx logs restart # restart nginx add # add subdomain link # connect subdomain with service edit # edit subdomain help # print help ```","title":"Setup"},{"location":"references/devops/domain_deploy/#add-subdomain","text":"Add subdomain to the services.ini file. ./please add test example.com","title":"Add subdomain"},{"location":"references/devops/domain_deploy/#link-subdomain","text":"Link service ./please link test Output: subdomain test.example.com is successfully link to docker with IP: 172.4.0.4 Use the IP as domain_ip in your service.","title":"Link subdomain"},{"location":"references/devops/server_config/","text":"Server Config & Orchestration Currently, our servers are configured manually and without any form of central configuration management. Nonetheless, for the future we have an ansible repository at viva-con-agua/ansible which we sometimes use for new configuration. How the ansible repository can be used to author and deploy changes is documented in the repository itself. Kubernetes Configuration The configuration and deployment of Kubernetes itself is done via kubespray which is configured from the aforementioned ansible repository . See that repository for exact details but generally, we use one control plane node and two workers, calico for networking and cri-o as container runtime. Installed Cluster Apps In addition to the bare kubernetes cluster we also have some applications deployed on it which make development and operations a lot easier. The following applications are currently deployed: App Description Access Link deploy-repo Ingress Nginx Kubernetes Ingress implementation (basically a reverse proxy server for other in-cluster services) n/a k8s-ingress-deploy cert-manager Automated TLS Certificate management (think certbot but kubernetes) n/a cert-manager-deploy Local Path Provisioner Dynamic provisioning of persistent local node storage to kubernetes workloads via PersistentVolumeClaims n/a k8s-local-path-provisioner-deploy ArgoCD Continuous Delivery engine which allows us to do git-ops style deployment of our other apps. See specific documentation page . click argocd-deploy","title":"Server Config & Orchestration"},{"location":"references/devops/server_config/#server-config-orchestration","text":"Currently, our servers are configured manually and without any form of central configuration management. Nonetheless, for the future we have an ansible repository at viva-con-agua/ansible which we sometimes use for new configuration. How the ansible repository can be used to author and deploy changes is documented in the repository itself.","title":"Server Config &amp; Orchestration"},{"location":"references/devops/server_config/#kubernetes-configuration","text":"The configuration and deployment of Kubernetes itself is done via kubespray which is configured from the aforementioned ansible repository . See that repository for exact details but generally, we use one control plane node and two workers, calico for networking and cri-o as container runtime.","title":"Kubernetes Configuration"},{"location":"references/devops/server_config/#installed-cluster-apps","text":"In addition to the bare kubernetes cluster we also have some applications deployed on it which make development and operations a lot easier. The following applications are currently deployed: App Description Access Link deploy-repo Ingress Nginx Kubernetes Ingress implementation (basically a reverse proxy server for other in-cluster services) n/a k8s-ingress-deploy cert-manager Automated TLS Certificate management (think certbot but kubernetes) n/a cert-manager-deploy Local Path Provisioner Dynamic provisioning of persistent local node storage to kubernetes workloads via PersistentVolumeClaims n/a k8s-local-path-provisioner-deploy ArgoCD Continuous Delivery engine which allows us to do git-ops style deployment of our other apps. See specific documentation page . click argocd-deploy","title":"Installed Cluster Apps"},{"location":"references/devops/server_overview/","text":"Server Overview This section of the documentation aims to describe which servers we have, how they are configured and what is running on them. List of Servers Name Hoster Hostname Kubernetes Role VcA CRM Robhost crm.srv.vivaconagua.org VcA Stage Hetzner stage.srv.vivaconagua.org VcA Pool Hetzner pool.srv.vivaconagua.org Worker VcA Live Hetzner live.srv.vivaconagua.org Worker VcA Backend Hetzner backend.srv.vivaconagua.org VcA Production Hetzner production.srv.vivaconagua.org Control Plane VcA BI Server Hetzner bi.srv.vivaconagua.org IPAM We do not have a dedicated tool for IP Address Management (IPAM) but this section documents which IP addresses we use where, why and which scheme they are assigned from. Public Server IPs Our Servers naturally have IP addresses of their own that are assigned from the respective hosting provider. Info Servers that have an IPv6 prefix defined have the whole prefix routed to them by the hosting provider but only have the given address actually allocated. Name IPv4 IPv6 VcA CRM 212.83.50.133 unknown VcA Stage 176.9.147.57 2a01:4f8:160:300b::2/64 VcA Pool 5.9.31.149 2a01:4f8:161:1386::2/64 VcA Live 144.76.62.66 2a01:4f8:191:7147::2/64 VcA Backend 136.243.53.198 2a01:4f8:212:14e1::2/64 VcA Production 136.243.50.228 2a01:4f8:212:12e2::2/64 VcA BI Server 5.9.57.115 2a01:4f8:161:5375::2/64","title":"Server Overview"},{"location":"references/devops/server_overview/#server-overview","text":"This section of the documentation aims to describe which servers we have, how they are configured and what is running on them.","title":"Server Overview"},{"location":"references/devops/server_overview/#list-of-servers","text":"Name Hoster Hostname Kubernetes Role VcA CRM Robhost crm.srv.vivaconagua.org VcA Stage Hetzner stage.srv.vivaconagua.org VcA Pool Hetzner pool.srv.vivaconagua.org Worker VcA Live Hetzner live.srv.vivaconagua.org Worker VcA Backend Hetzner backend.srv.vivaconagua.org VcA Production Hetzner production.srv.vivaconagua.org Control Plane VcA BI Server Hetzner bi.srv.vivaconagua.org","title":"List of Servers"},{"location":"references/devops/server_overview/#ipam","text":"We do not have a dedicated tool for IP Address Management (IPAM) but this section documents which IP addresses we use where, why and which scheme they are assigned from.","title":"IPAM"},{"location":"references/devops/server_overview/#public-server-ips","text":"Our Servers naturally have IP addresses of their own that are assigned from the respective hosting provider. Info Servers that have an IPv6 prefix defined have the whole prefix routed to them by the hosting provider but only have the given address actually allocated. Name IPv4 IPv6 VcA CRM 212.83.50.133 unknown VcA Stage 176.9.147.57 2a01:4f8:160:300b::2/64 VcA Pool 5.9.31.149 2a01:4f8:161:1386::2/64 VcA Live 144.76.62.66 2a01:4f8:191:7147::2/64 VcA Backend 136.243.53.198 2a01:4f8:212:14e1::2/64 VcA Production 136.243.50.228 2a01:4f8:212:12e2::2/64 VcA BI Server 5.9.57.115 2a01:4f8:161:5375::2/64","title":"Public Server IPs"},{"location":"references/packages/","text":"Packages Overview Viva con Agua maintains some packages that are used internally to ease software development. See the sidebar for a list of currently maintained packages. Their usage is described in each article.","title":"Packages Overview"},{"location":"references/packages/#packages-overview","text":"Viva con Agua maintains some packages that are used internally to ease software development. See the sidebar for a list of currently maintained packages. Their usage is described in each article.","title":"Packages Overview"},{"location":"references/packages/vite_runtime_config/","text":"vite-plugin-runtime-config We have authored and published a vite plugin to allow dynamic configuration of our Vue apps at runtime. It is documented in the repository at github.com/Viva-con-Agua/vite-plugin-runtime-config .","title":"vite-plugin-runtime-config"},{"location":"references/packages/vite_runtime_config/#vite-plugin-runtime-config","text":"We have authored and published a vite plugin to allow dynamic configuration of our Vue apps at runtime. It is documented in the repository at github.com/Viva-con-Agua/vite-plugin-runtime-config .","title":"vite-plugin-runtime-config"},{"location":"references/packages/vcago/","text":"vcago The package contains standard functions that are used in the Viva-con-Agua API services and is on the echo web framework .","title":"vcago"},{"location":"references/packages/vcago/#vcago","text":"The package contains standard functions that are used in the Viva-con-Agua API services and is on the echo web framework .","title":"vcago"},{"location":"references/packages/vcago/base_server/","text":"Basic Webserver Setup in server.go 1 2 3 4 5 6 7 8 9 10 11 12 func main() { e := echo.New() e.HTTPErrorHandler = vcago.HTTPErrorHandler e.Validator = vcago.JSONValidator e.Use(vcago.CORS.Init()) e.Use(vcago.Logger.Init()) ... ... appPort := vcago.Config.GetEnvString(\"APP_PORT\", \"n\", \"1323\") e.Logger.Fatal(e.Start(\":\" + appPort)) } edit the .env file 1 2 3 4 5 SERVICE_NAME=default //service name, default default APP_PORT=1323 // default port 1323 LOGGING_OUTPUT=strout // pretty, nats default strout ALLOW_ORIGINS=\"https://example.com,https://api.example.com\" ... output logger pretty 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"id\": \"\", //not implemented \"service\": \"example\", \"time\": \"2022-02-07T19:50:08.971851362+01:00\", \"remote_ip\": \"127.0.0.1\", \"host\": \"localhost:1323\", \"method\": \"POST\", \"uri\": \"/example\", \"user_agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:96.0) Gecko/20100101 Firefox/96.0\", \"status\": 500, \"error\": { \"status_message\": \"example error\", \"status_type\": \"internal_error\" }, \"latency\": 2002915, \"latency_human\": \"2.002915ms\", \"byte_in\": \"\", \"byte_out\": \"\", \"modified\": { \"updated\": 1644259808, \"created\": 1644259808 } }","title":"Base Server"},{"location":"references/packages/vcago/base_server/#basic-webserver","text":"","title":"Basic Webserver"},{"location":"references/packages/vcago/base_server/#setup-in-servergo","text":"1 2 3 4 5 6 7 8 9 10 11 12 func main() { e := echo.New() e.HTTPErrorHandler = vcago.HTTPErrorHandler e.Validator = vcago.JSONValidator e.Use(vcago.CORS.Init()) e.Use(vcago.Logger.Init()) ... ... appPort := vcago.Config.GetEnvString(\"APP_PORT\", \"n\", \"1323\") e.Logger.Fatal(e.Start(\":\" + appPort)) }","title":"Setup in server.go"},{"location":"references/packages/vcago/base_server/#edit-the-env-file","text":"1 2 3 4 5 SERVICE_NAME=default //service name, default default APP_PORT=1323 // default port 1323 LOGGING_OUTPUT=strout // pretty, nats default strout ALLOW_ORIGINS=\"https://example.com,https://api.example.com\" ...","title":"edit the .env file"},{"location":"references/packages/vcago/base_server/#output-logger-pretty","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"id\": \"\", //not implemented \"service\": \"example\", \"time\": \"2022-02-07T19:50:08.971851362+01:00\", \"remote_ip\": \"127.0.0.1\", \"host\": \"localhost:1323\", \"method\": \"POST\", \"uri\": \"/example\", \"user_agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:96.0) Gecko/20100101 Firefox/96.0\", \"status\": 500, \"error\": { \"status_message\": \"example error\", \"status_type\": \"internal_error\" }, \"latency\": 2002915, \"latency_human\": \"2.002915ms\", \"byte_in\": \"\", \"byte_out\": \"\", \"modified\": { \"updated\": 1644259808, \"created\": 1644259808 } }","title":"output logger pretty"},{"location":"tutorials/","text":"Introducing Tutorials This section of the documentation aim to teach a new subject from scratch. Everything here is written in easy language and allows newcomers to get started. Missing There are not yet any tutorials. Remove this note if you add an explanation.","title":"Introducing Tutorials"},{"location":"tutorials/#introducing-tutorials","text":"This section of the documentation aim to teach a new subject from scratch. Everything here is written in easy language and allows newcomers to get started. Missing There are not yet any tutorials. Remove this note if you add an explanation.","title":"Introducing Tutorials"}]}